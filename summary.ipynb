{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# blog_finder$^{\\dagger}$: an application to find the best blogs, bloggers, and blog posts on any given topic\n",
    "- Crawls technical internet communities looking for links to blog posts\n",
    "- Returns the blogs that are most relevant to user and most highly regarded by the community\n",
    "\n",
    "$^{\\dagger}$: application title subject to change :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary and workflow for blog finder application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary:\n",
    "1. Raw data acquisition:\n",
    "    - Store raw data from posts from various social communities\n",
    "    - Sources:\n",
    "        1. __Reddit__: Make calls to Reddit API via custom Python script\n",
    "        2. __StackExchange__: Use StackExchange API with custom Python script\n",
    "        3. __Quora__:\n",
    "        4. __Twitter__:\n",
    "\n",
    "2. Blog post URL detection:\n",
    "    - First find links from regex\n",
    "    - Determine if link points to a blog\n",
    "        - Binary classification problem? (Is blog vs. is not blog?) Need to collect features on training data (pages that are known to be blog or not)\n",
    "     \n",
    "3. Data storage\n",
    "    - Each data source gets its own table, reflecting the different features each community has\n",
    "    - For example, for reddit the features might be\n",
    "        1. Post ID (the table's *primary key*)\n",
    "        2. Parent title\n",
    "        3. Date/time\n",
    "        4. Link\n",
    "        5. Upvotes\n",
    "        6. Blog contents\n",
    "  \n",
    "4. Utility metrics\n",
    "    1. __Relevance score__: A measure of how relevant the blog post is to the user's query\n",
    "        - Determined by tf-idf distance between query and community, blog post content, linking post content, etc.\n",
    "    2. __Epistemic score__: A measure of the educational utility of the post\n",
    "        - Determined by upvotes, frequency of appearance, etc.\n",
    "    \n",
    "5. Serve (one, two, three...) such posts to the user (daily, weekly...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1. Raw data acquisition\n",
    "\n",
    "### Corpus summary\n",
    "\n",
    "- Data used to-date from Reddit and StackOverflow\n",
    "- Entire corpus is 700 MB in size and contains 20,000 links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i. Reddit comments\n",
    "- Use Reddit API to download entire subreddits worth of comments at a time.\n",
    "- Search by date, t1 to t2 on a given subreddit for all submissions that occurred within those bounds.\n",
    "- Search within those submissions and accumulate all comments.\n",
    "- Save raw data to a plain text file, with one file per subreddit.\n",
    "- Add submissions and comments raw data with all relevant features to a MySQL database.\n",
    "- When searching all submissions and comments to a subreddit, first check the database to see if it is already added; this avoids the need to make another costly call to the API.\n",
    "\n",
    "#### Features:\n",
    "1. id (primary key)\n",
    "2. date created utc\n",
    "3. body of submission/comment\n",
    "4. upvotes\n",
    "5. url\n",
    "\n",
    "#### Subreddit info\n",
    "1. /r/MachineLearning\n",
    "    - 30,260 comments\n",
    "    - 2,830 links\n",
    "    \n",
    "2. /r/datascience\n",
    "    - 17,362 comments\n",
    "    - 2,193 links\n",
    "\n",
    "3. /r/compsci\n",
    "    - 23,847 comments\n",
    "    - 3,464 links\n",
    "4. /r/statistics\n",
    "    - 24,232 comments\n",
    "    - 1,609 links\n",
    "    \n",
    "5. /r/physics\n",
    "    - 35,744 comments\n",
    "    - 3,131 links\n",
    "\n",
    "5. /r/Cooking\n",
    "    - 52,923 comments\n",
    "    - 2,863 links\n",
    "    \n",
    "7. /r/food\n",
    "    - 29,048 comments\n",
    "    - 930 links    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii. StackExchange comments\n",
    "- Use StackExchange API to download entire community's questions and answers at a time.\n",
    "- Save raw data to two different plain text files: '[community]_questions' and '[community]_answers'\n",
    "- Add data with relevant features to a MySQL database.\n",
    "\n",
    "#### Features:\n",
    "- Questions:\n",
    "    - id (primary key)\n",
    "    - date created utc\n",
    "    - body\n",
    "    - upvotes\n",
    "    - url\n",
    "    \n",
    "- Answers:\n",
    "    - id (primary key)\n",
    "    - date created utc\n",
    "    - body\n",
    "    - upvotes\n",
    "    - url\n",
    "    - parent\n",
    "    \n",
    "#### Community info:\n",
    "- datascience\n",
    "    - 6,373 questions\n",
    "    - 13,3236 answers\n",
    "- stackoverflow\n",
    "    - 212,796 questions\n",
    "    - 57,696 answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Match search topic to community\n",
    "- A community is represented by a word frequency vector, which is a dict in Python\n",
    "     - e.g. /r/MachineLearning might look like {'machine': 103230, 'learning': 10283, 'regression': 10280:, ...}\n",
    "- The search term is also represented by such a dict\n",
    "    - e.g. for searching 'neural network', dict is {'machine': 0, ..., 'neural': 1, ... 'network': 1, ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Finding links:\n",
    "- Fitness score=(community relevance)x(post relevance)x(normalized upvotes)\n",
    "- community relevance: (normalized community topic vector) . (normalized search topic vector)\n",
    "- post relevance: (normalized post contents topic vector) . (normalized search topic vector)\n",
    "        - Maybe add \n",
    "- Normalized upvotes: Post upvotes/total submission upvotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Plot ideas\n",
    "1. Most common words by subreddit\n",
    "    - Have e.g. /r/MachineLearning and /r/datascience on same plot, with the x axis labels the words themselves and upper axis can be one subreddit, lower axis the other\n",
    "2. Random relevant search terms for given topics, and the cosine distance score for a given subreddit\n",
    "    - Make this a histogram, where each term gets number of bars equal to number of subreddits\n",
    "    - For instance, could use /r/Datascience, /r/MachineLearning, /r/Statistics, etc...\n",
    "3. Matrix of cosine distance (dot product) of word vectors for different subreddits\n",
    "    - Need a spectrum of highly related, fairly related, and not related subreddits\n",
    "    - e.g., /r/MachineLearning, /r/datascience, /r/statistics, /r/CompSci, /r/Cooking, /r/food"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
