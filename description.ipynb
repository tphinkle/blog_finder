{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# blog_finder$^{\\dagger}$: an application to find the best blogs, bloggers, and blog posts on any given topic\n",
    "- Crawls technical internet communities looking for links to blog posts\n",
    "- Returns the blogs that are most relevant to user and most highly regarded by the community\n",
    "\n",
    "$^{\\dagger}$: application title subject to change :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progress as of 5-3-2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data acquisition\n",
    "    - I Wrote Python wrappers for reddit and StackOverflow api's that allow for full-history downloads of any sub-community on those sites (i.e., subreddits and StackOverflow sites)\n",
    "    - The corpus so far is ~1 GB worth of data from select subreddits and stackexchange sites, with more readily available after making calls to the api's with the written code.\n",
    "    - The communities were selected on the basis of having a wide range of degrees of similarity\n",
    "        -e.g., /r/datascience and stackexchange.datascience are very similar, /r/datascience and /r/statistics are somewhat similar, and /r/datascience and /r/Cooking are not similar.\n",
    "        \n",
    "        - subreddits:\n",
    "            1. /r/MachineLearning\n",
    "                - 30,260 comments\n",
    "                - 2,830 links\n",
    "    \n",
    "            2. /r/datascience\n",
    "                - 17,362 comments\n",
    "                - 2,193 links\n",
    "\n",
    "            3. /r/compsci\n",
    "                - 23,847 comments\n",
    "                - 3,464 links\n",
    "                \n",
    "            4. /r/statistics\n",
    "                - 24,232 comments\n",
    "                - 1,609 links\n",
    "\n",
    "            5. /r/physics\n",
    "                - 35,744 comments\n",
    "                - 3,131 links\n",
    "\n",
    "            5. /r/Cooking\n",
    "                - 52,923 comments\n",
    "                - 2,863 links\n",
    "\n",
    "            7. /r/food\n",
    "                - 29,048 comments\n",
    "                - 930 links\n",
    "                \n",
    "        - stackexchange sites:\n",
    "            1. stackexchange.datascience\n",
    "                - 139,619 comments\n",
    "                \n",
    "            2. stackoverflow\n",
    "                - 271,492 comments\n",
    "                - 28,076 links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Community exploratory data analysis\n",
    "    \n",
    "    - Before building the recommender engine, I wanted to explore individual and shared aspects of different communities.\n",
    "    \n",
    "    - First, I looked at how often blog posts are actually shared on a particular subreddit; it's no use in doing this if no one ever links to blogs! Fortunately, it looks like blogs are shared sufficiently often that the recommender engine will have enough data to work with.\n",
    "![https://raw.githubusercontent.com/tphinkle/blog_finder/master/plots/blog_timeseries.png](./plots/blog_timeseries.png)\n",
    "\n",
    "    - Next, I wanted to see how often a community linked to the same blogger.\n",
    "![https://raw.githubusercontent.com/tphinkle/blog_finder/master/plots/links_to_blogs.png](./plots/links_to_blogs.png)\n",
    "\n",
    "    - Sometimes there are multiple communities based around topics that are similar. I wanted to see if two communities based on similar topics had similar post titles. For the two communities, I looked at /r/datascience and /r/MachineLearning and plotted the top twenty most common words posted to each community. As expected, there was a fair amount of overlap in the two; for instance, 7 words are found in the top twenty search terms in both communities.\n",
    "![https://raw.githubusercontent.com/tphinkle/blog_finder/master/plots/most_common_terms.png](./plots/most_common_terms.png)\n",
    "\n",
    "    - In order to efficiently determine which communities we should be looking in for blogs to recommend, it's important to have a well-defined metric for similarity of the desired search topic and the community topic. To do this, I first created dictionaries (basically, vectors) of word counts from the search query and from every single post title submitted to a given subcommunity. The cosine-distance similarity metric is basically just a normalized dot product of two such vectors, with resultant numbers close to 1 indicating high similarity. Here are the similarity scores for searches that I made to communities that I thought would show high relevance.\n",
    "![https://raw.githubusercontent.com/tphinkle/blog_finder/master/plots/search_term_relevance.png](./plots/search_term_relevance.png)\n",
    "\n",
    "    - Finally, I wanted to investigate the degree of similarity between the communities that I chose. I used the same similarity metric as in the previous comment, and plotted the similarities in a matrix. Not surprisingly, the communities that we expect to be similar are similar as shown in the following plot.\n",
    "![https://raw.githubusercontent.com/tphinkle/blog_finder/master/plots/community_similarities.png](./plots/community_similarities.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Getting the blog recommendations\n",
    "\n",
    "- Now that we have a clearly defined way of investigating which communities we should be searching for blogs for a given search query in, let's give it a shot!\n",
    "\n",
    "- For this example, I am searching for the topic 'neural network', and for the preliminary analysis I am artificially automatically picking to search for posts in three communities I already know are relevant. Once deployed, the application will determine the appropriate communities to look in. Here are the three communities.\n",
    "    - /r/MachineLearning\n",
    "    - /r/datascience\n",
    "    - stackexchange.datascience\n",
    "    \n",
    "- For each community, I extracted every single comment. From those comments, I first filtered out those that contained urls using regex's. Then, I filtered those comments again, retaining only those that had 'blog' in the url. There are probably a lot of great links that were filtered out in this last step, so the final application will need to be more savvy in finding out which links point to blogs.\n",
    "\n",
    "- Once I had all the comments containing blog links, I assigned each link two scores:\n",
    "    1. A 'utility' score based on how useful the community found that post (and hence the blog link) as measured by votes.\n",
    "    2. A 'relevance' score, based on the same cosine-similarity metric described previously, with the two vectors being the search query and the post contents.\n",
    "    \n",
    "- Here is a scatter plot of the 'utility' and 'relevance' scores for the search query 'neural network'. I added a decision boundary to the plot, where points within the decision boundary are *not* recommended, and those outside *are* recommended. The blog links could then be sorted, say, by normal distance to the decision boundary.\n",
    "![https://raw.githubusercontent.com/tphinkle/blog_finder/master/plots/neural_network_query.png](./plots/neural_network_query.png)\n",
    "    \n",
    "- Finally, let's see if our little blog recommender works. Hopefully we should find that the blogs that the links point to are both well-written and related to neural networks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
